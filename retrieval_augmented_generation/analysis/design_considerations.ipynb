{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Considerations\n",
    "\n",
    "## Part 1: Identify FIVE design decisions for this system.\n",
    "Argue for how this design decision is significant to the system, and your plans to analyze this design. If needed, use the dataset provided to you.\n",
    "\n",
    "### 1. Sentences per Chunk\n",
    "One important factor to consider is the number of sentences per chunk, which is used by the `DocumentProcessing` class to split the corpus documents into smaller chunks before the pipeline calculates the embedding vector for each chunk. During the retrieval and generation process, these chunks will be used as context and provided as input to the generator LLM that generates the answer to the question. Therefore, the number of sentences in these chunks will directly affect how much contextual information the generator will have available to generate a response. A high number of sentences per chunk provides the generator with lots of contextual information, which might be useful for complex questions. However, too much information may result in irrelevant information being provided to the generator, which may confuse the model and produce unreliable answers. A small number of sentences per chunk could decrease the chances of providing irrelevant information, but could also decrease the chances of including the true answer in the context.\n",
    "\n",
    "**Analysis Plan**\n",
    "To analyze the effects of different numbers of sentences per chunk, a validation set of questions, like the ones in [qa_resources/questions.csv](../qa_resources/questions.csv), could be used to test how the RAG system performs when the corpus documents have been processed using different numbers of sentences per chunk. For example we could use sentence per chunk values of [1,2,3,4,5,6] and for each value, we can calculate the document embeddings using the specified sentences per chunk, and use those embeddings to retrieve the context to be used to generate an answer for each question in the validation set. The answers to each question can be evaluated using automated metrics such as exact match and transformer match metrics. Additionally, the time it takes to get the context and generate an answer for a question can be tracked to identify any potential differences in latency for different sentences per chunk values. The sentences per chunk value that results in the best balance of high transformer match and low latency would be chosen for deployment to the system. Transformer match metric is prioritized over exact match because it allows for some flexibility in phrasing, paraphrasing, or use of synonyms in the generated response.\n",
    "\n",
    "### 2. Similarity Measure\n",
    "Another important design consideration is the similarity measure used in the Nearest Neighbor search when calculating the distance between two embedding vectors during the context retrieval process. This is important because it can affect how relevant the retrieved context is to a given question. The distance metric must ensure that higher distances are associated with dissimilar text and lower distances are associated with similar text strings. Different distance metrics can lead to different results for the same points, so it's important to carefully choose the distance measure that results in the best system performance. Some popular distance metrics include Manhattan distance (aka \"city block\" distance), Euclidean distance (aka \"straight line\" distance), Minkowski distance (a generalized version of Manhattan and Euclidean distances), and cosine similarity which measures the angle between two vectors.\n",
    "\n",
    "**Analysis Plan**\n",
    "\n",
    "To analyze the different similarity measures, each variation (Manhattan distance, Euclidean distance, Minkwoski distance with different values for the p parameter, and cosine similarity) can be used to retrieve context used to generate answers for validation set of questions with known answers. The exact match and transformer match metrics can then be calculated for each run, so that the quality of the system outputs can be compared for the context search using each similarity measure. Additionally, the mean search time per question can be tracked so we can identify any differences in efficiency between the similarity measures. The similarity measure resulting in the best balance of transformer match and efficiency would be chosen for deployment. Note that for the analysis, the transformer match is calculated using the output of the generator that creates a response using the context identified using the KNN search. This does not directly evaluate the outputs of the KNN search, but additional analysis couold be done to evaluate the nearest neighbors directly. This could be done by calculating the quality of the ranked nearest samples by finding the mean avereage precision, where relevant matches are determined by the transformer match matric applied to the nearest neighbor chunk and the ground truth answer. This analysis would more directly determine how the similarity measure affects the relevance of the retrieved context chunks.\n",
    "\n",
    "### 3. Number of Nearest Neighbors (K)\n",
    "Another important design consideration is the number of nearest neighbors (k value) to be used in the search service when retrieving relevant context chunks for a given question. Lower k values may be more sensitive to noisy data (overlap between chunks of corpus documents in our case), but can also be useful to account for complex decision boundaries between clusters of document chunks. Higher k values produce results that are less affected by outliers or overlap in document chunk clusters, but can introduce bias from irrelevant sentences that may become included when we expand k. Similar to the sentences per chunk analysis, a small k value could decrease the chances of providing irrelevant information, but could also decrease the chances of including the true answer in the context. Ideally, k should be large enough to capture chunks containing the true answer to the question, but small enough to exclude irrelevant information as much as possible.\n",
    "\n",
    "**Analysis Plan**\n",
    "\n",
    "To find the best k value, the search service can be run on a validation set of questions, using a different k value for each run. For each run, for each question, the k retrieved context chunks will then be used as input with the question to the generater that will create an answer to the question. The answers can be evaluated using the exact match and transformer match metrics for each run, so that the quality of the answers can be compared for each k value. Additionally, mean search time per question can be tracked so we can identify any differences in efficiency for different k values. The k value resulting in the best balance of transformer match metric and search time would be chosen for deployment.\n",
    "\n",
    "### 4. Embedding Model Selection\n",
    "Another important design consideration is the embedding model used to generate text embedding vectors from the corpus documents and input questions. The `all-MiniLM-L6-v2` sentence transformer model was provided in the case study as an example, with additional pretrained sentence transformer models described at [https://sbert.net/docs/sentence_transformer/pretrained_models.html](https://sbert.net/docs/sentence_transformer/pretrained_models.html). Each model produces different embeddings, leading to differences in cluster separability of text chunks and documents, leading to differing KNN search results, resulting in different contextual information and therefore generated responses. Models that produce embeddings that put text chunks of the same topic (i.e. from the same document) closer together, and text of different topics further apart (i.e. models that produce well-defined clusters of text) will ultimately lead to the most relevant context retrieved and best generated answer results. Additionally, the different embedding models all have different sizes, speeds, and accuracies. Typically, the larger models perform better but may be slower. Smaller models can be quicker and faster, but may perform worse. Furthermore, the models produce embedding vectors of different sizes, which is important to consider since it will affect our system's storage requirements since we are saving the embedding vectors for each document chunk. Additionally, smaller embedding vectors are more computationally efficient, but contain less information and therefore may not fully represent key aspects that differentiate each text chunk. Larger embedding vectors are more computationally intensive to search, but contain more information and may represent more complex aspects that differentiate text. However, larger embedding vectors may also contain more extraneous, or noisy information unrelated to the text topic such as specific punctuation or phrasing information. Ideally, the embedding vectors should be large enough to capture key aspects of the text that are significant for differentiating between ideas, but small enough so they don't contain too much noisy information that isn't important for a sentence (e.g. punctuation or paraphrasing).\n",
    "\n",
    "**Analysis Plan**\n",
    "\n",
    "To analyze and compare each model, embeddings can be computed for the corpus documents using a few different models such as `all-MiniLM-L6-v2`, `all-mpnet-base-v2`, `all-MiniLM-L12-v2`, and `paraphrase-MiniLM-L3-v2`. The clustering quality of the resulting embeddings can be measured using silhouette score. A higher silhouette score indicates better intra-cluster cohesion and inter-cluster separation. Therefore, a higher silhouette score indicates that using the model in our system will result in better clustering of document text leading to more relevant context retrieval and ideally better generated answers. To further support this, a validation set of known questions can be run through our RAG system using the embeddings produced by each of the 4 models, and the transformer match metric can be calculated for the generated answer produced using the context retrieved from the KNN search using the embedding vectors produced by each model. Additionally, response time per question can be measured to analyze any potential differences in efficiency of using the embeddings produced by each model. The model that leads to the highest transformer match value with an efficient response time is the one that should be chosen for deployment.\n",
    "\n",
    "### 5. Generator Model Selection\n",
    "Another important design consideration is the generator model used to produce an answer to a given question using the provided context.\n",
    "\n",
    "**Analysis Plan**\n",
    "\n",
    "## Additional Design Considerations\n",
    "Additional considerations beyond the minimum 5 required.\n",
    "\n",
    "### 6. Search Technique\n",
    "Choosing a search technique is an important design consideration for the retrieval service in the system. The search module is responsible for searching the database of known embeddings (from a corpus of documents) to find the most relevant context chunks to the input question. This search needs to be accurate so that we are able to provide the generator model with contextual knowledge that includes the information needed to generate aa correct response, but the search also needs to be efficient so that users don't have to wait for long periods of time for the system to generate an answer to a simple question. Even if we decide that we'll use K-Nearest Neighbors for our search algorithm, there are still two main aspects of the search module to consider: exact vs. approximate search and indexing strategy. Implementing some kind of indexing is important because it allows the data to be organized in a way that makes searching, inserting, deleting, and retrieval more efficient.\n",
    "- **Exact vs. Approximate:** Exact searches like brute force can be highly accurate and can guarantee that the returned samples are the most relevant to the input question; however, this can be computationally expensive, especially if the embeddings database has many samples or high dimensionality. Approximate searches don't require visiting every single sample in the embeddings database, and therefore are more computationally efficient; however, may be less accurate since they don't guarantee that the optimal solution is identified, since they only search for an adequate solution that's good enough.\n",
    "- **Indexing Strategies:** Indexing strategies organize the data in a way the improves efficiency for search and retrieval. Some strategies like KD trees can be applied to both exact and approximate searches, while others like local sensitivity hashing (LSH) and hierarchical navigable small world graphs (HNSW) are intended for approximate searches. KD trees partition the data by dimension into \"hyperrectangles\", allowing for more efficient search since some branches can be eliminated; however, tree depth increases with dimensionality, making this less efficient for high dimensional data. LSH groups similar items into buckets by increasing the probability of hashing collisions for similar items, and is a good alternative for high dimensional data; however, may result in false negatives due to its probablistic nature. HNSW is an accurate, scalable and efficient solution for high dimensional data, where the data is organized into heirarchical layers, with each course layers first and more fine-grained layers towards the bottom; however, storing a graph of all of the layers can be memory intensive.\n",
    "\n",
    "**Analysis Plan**\n",
    "\n",
    "To analyze the different search techniques, each variation (brute force, exact KNN,KD trees, LSH, HNSW) can be run on a validation set of questions with known answers. The transformer match metric can be calculated for the generated answer produced using the context retrieved from the KNN search for each search technique. Additionally, the mean search time per question will be tracked, so we can determine which technique is the most efficient. The search technique resulting in the best balance of the transformer match metric and efficiency would be chosen for deployment.\n",
    "\n",
    "### References\n",
    "1. Pham, A. (2022, April 20). A look into K-Dimensional Trees. Medium. https://medium.com/smucs/a-look-into-k-dimensional-trees-290ec69dffe9\n",
    "2. MyScale. (2023, June 14). HNSW vs KNN: The Ultimate Algorithm Showdown. https://myscale.com/blog/hnsw-vs-knn-algorithm-comparison/\n",
    "3. Ng, A. (2018, April 16). kNN.16 Locality sensitive hashing (LSH) [Video]. YouTube. https://www.youtube.com/watch?v=LqcwaW2YE_c\n",
    "4. Kaushik, S. (2021, January 11). Importance of Distance Metrics in Machine Learning Modelling. Towards Data Science. https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d\n",
    "5. Gokte, S. A. (2020, November 17). Most Popular Distance Metrics Used in KNN and When to Use Them. KDnuggets. https://www.kdnuggets.com/2020/11/most-popular-distance-metrics-knn.html\n",
    "6. A. Sharma, \"How to find the optimal value of K in KNN?,\" Towards Data Science, 21-Oct-2020. [Online]. Available: https://towardsdatascience.com/how-to-find-the-optimal-value-of-k-in-knn-35d936e554eb. [Accessed: 28-Jul-2024].\n",
    "\n",
    "## Part 2: Analyze TWO out of the five design decisions. Add the results of your analysis.\n",
    "\n",
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentences per Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generator Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional: Embedding Model Selection, Number of Nearest Neighbors (K), Similarity Measure"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
